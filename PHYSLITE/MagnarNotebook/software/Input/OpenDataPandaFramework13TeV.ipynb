{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot as uproot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir, walk\n",
    "from os.path import isfile, join, isdir\n",
    "import os\n",
    "import h5py\n",
    "import awkward as awkward\n",
    "#import awkward0 as awkward0\n",
    "import time\n",
    "from itertools import combinations\n",
    "import sys\n",
    "import vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This library contains handy functions to ease the access and use of the 13TeV ATLAS OpenData release\n",
      "\n",
      "getBkgCategories()\n",
      "\t Dumps the name of the various background cataegories available \n",
      "\t as well as the number of samples contained in each category.\n",
      "\t Returns a vector with the name of the categories\n",
      "\n",
      "getSamplesInCategory(cat)\n",
      "\t Dumps the name of the samples contained in a given category (cat)\n",
      "\t Returns dictionary with keys being DSIDs and values physics process name from filename.\n",
      "\n",
      "getMCCategory()\n",
      "\t Returns dictionary with keys DSID and values MC category\n",
      "\n",
      "initialize(indir)\n",
      "\t Collects all the root files available in a certain directory (indir)\n",
      "\n",
      "getSkims(indir)\n",
      "\t Prints all available skims in the directory\n",
      "\n",
      "\n",
      "\n",
      "Setting luminosity to 10064 pb^-1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"This library contains handy functions to ease the access and use of the 13TeV ATLAS OpenData release\")\n",
    "print(\"\\ngetBkgCategories()\")\n",
    "print(\"\\t Dumps the name of the various background cataegories available \\n\\t as well as the number of samples contained in each category.\\n\\t Returns a vector with the name of the categories\")\n",
    "print(\"\\ngetSamplesInCategory(cat)\")\n",
    "print(\"\\t Dumps the name of the samples contained in a given category (cat)\\n\\t Returns dictionary with keys being DSIDs and values physics process name from filename.\")\n",
    "print(\"\\ngetMCCategory()\")\n",
    "print(\"\\t Returns dictionary with keys DSID and values MC category\")\n",
    "print(\"\\ninitialize(indir)\")\n",
    "print(\"\\t Collects all the root files available in a certain directory (indir)\")\n",
    "print(\"\\ngetSkims(indir)\")\n",
    "print(\"\\t Prints all available skims in the directory\")\n",
    "print(\"\\n\\n\")\n",
    "print(\"Setting luminosity to 10064 pb^-1\\n\")\n",
    "lumi = 10064.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lists defines the available categories for the SM backgrounds and signal available in the openData 13 TeV release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcfiles = []\n",
    "datafiles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"Background_samples_13TeV.txt\"):\n",
    "    infofile = open(\"Background_samples_13TeV.txt\", \"r\")\n",
    "else:\n",
    "    infofile = open(sys.path[1]+\"/Input/Background_samples_13TeV.txt\", \"r\")\n",
    "bkg_dsid_toplot = {}\n",
    "sig_dsid_toplot = {}\n",
    "for line in infofile:\n",
    "    if line.startswith(\"#\"): continue\n",
    "    try:\n",
    "        fname,dsid,cat = line.split()\n",
    "    except:\n",
    "        continue\n",
    "    bkg_dsid_toplot[fname] = {\"cat\":cat,\"DSID\":int(dsid)}\n",
    "    \n",
    "infofile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"Signal_samples_13TeV.txt\"):\n",
    "    infofile = open(\"Signal_samples_13TeV.txt\", \"r\")\n",
    "else:\n",
    "    infofile = open(sys.path[1]+\"/Input/Signal_samples_13TeV.txt\", \"r\")\n",
    "for line in infofile:\n",
    "    if line.startswith(\"#\"): continue\n",
    "    try:\n",
    "        fname,dsid,cat = line.split()\n",
    "    except:\n",
    "        continue\n",
    "    sig_dsid_toplot[fname] = {\"cat\":cat,\"DSID\":int(dsid)}\n",
    "infofile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(indir):\n",
    "    filedic = {}\n",
    "    files = [f for f in listdir(indir) if (isfile(join(indir, f)) and f.endswith(\".root\"))]\n",
    "    isFound = {}\n",
    "    for f in files:\n",
    "        file_not_in_cat = True\n",
    "        ident = f.split(\".\")[1]\n",
    "        cat = \"\"\n",
    "        typ = \"\"\n",
    "        for key in bkg_dsid_toplot.keys():\n",
    "            if not key in isFound.keys():\n",
    "                isFound[key] = False\n",
    "            if key == ident:\n",
    "                cat = bkg_dsid_toplot[key][\"cat\"]\n",
    "                typ = \"bkg\"\n",
    "                isFound[key] = True\n",
    "                file_not_in_cat = False\n",
    "                break\n",
    "        for key in sig_dsid_toplot.keys():\n",
    "            if not key in isFound.keys():\n",
    "                isFound[key] = False\n",
    "            if key == ident:\n",
    "                cat = sig_dsid_toplot[key][\"cat\"]\n",
    "                typ = \"signal\"\n",
    "                isFound[key] = True\n",
    "                file_not_in_cat = False\n",
    "                break\n",
    "        if \"data_\" in f and not cat:\n",
    "            cat = \"data\"\n",
    "            typ = \"data\"\n",
    "            file_not_in_cat = False\n",
    "        if cat:\n",
    "            if not cat in filedic:\n",
    "                filedic[cat] = {\"files\":[],\"type\":typ,\"dsid\":[]}\n",
    "            filedic[cat][\"files\"].append(indir+\"/\"+f)\n",
    "            filedic[cat][\"dsid\"].append(f.split(\".\")[0].split(\"_\")[-1])\n",
    "        else: # the hadded files have cat in their name (i.e. <cat>.root)\n",
    "            cat = f.split(\".\")[0]\n",
    "            if cat:\n",
    "                if not cat in filedic:\n",
    "                    filedic[cat] = {\"files\":[],\"type\":typ,\"dsid\":[]}\n",
    "                filedic[cat][\"files\"].append(indir+\"/\"+f)\n",
    "                filedic[cat][\"dsid\"].append(f.split(\".\")[0].split(\"_\")[-1])\n",
    "            else:\n",
    "                print(\"WARNING \\t Could not find category for %s\"%ident)\n",
    "        if file_not_in_cat:\n",
    "            print(\"WARNING \\t File %s not added as sample in Background_samples_13TeV.txt/Signal_samples_13TeV.txt\"%(f))\n",
    "            \n",
    "    if not cat == 'data':\n",
    "        bkghead = False\n",
    "        sighead = False\n",
    "        sigstr = \"\"\n",
    "        bkgstr = \"\"\n",
    "        for key in isFound.keys():\n",
    "            if key in sig_dsid_toplot.keys():\n",
    "                if not isFound[key]:\n",
    "                    if not sighead:\n",
    "                        sigstr = \"#\"*100\n",
    "                        sigstr += \"\\n\"\n",
    "                        sigstr += \"SIGNAL SAMPLES\\n\"\n",
    "                        sigstr += \"#\"*100\n",
    "                        sigstr += \"\\n\"\n",
    "                        sighead = True\n",
    "                    sigstr += \"WARNING \\t File for %s not found in %s\\n\"%(key,indir)\n",
    "            elif key in bkg_dsid_toplot.keys():\n",
    "                if not isFound[key]:\n",
    "                    if not bkghead:\n",
    "                        bkgstr = \"#\"*100\n",
    "                        bkgstr += \"\\n\"\n",
    "                        bkgstr += \"BACKGROIUND SAMPLES\\n\"\n",
    "                        bkgstr += \"#\"*100\n",
    "                        bkgstr += \"\\n\"\n",
    "                        bkghead = True\n",
    "                    bkgstr += \"WARNING \\t File for %s not found in %s\\n\"%(key,indir)\n",
    "        if sighead:\n",
    "            print(sigstr)\n",
    "        if bkghead:\n",
    "            print(bkgstr)\n",
    "    return filedic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSignalCategories():\n",
    "    newdic = {}\n",
    "    for key in sig_dsid_toplot.keys():\n",
    "        if not sig_dsid_toplot[key][\"cat\"] in newdic.keys():\n",
    "            newdic[sig_dsid_toplot[key][\"cat\"]] = 0\n",
    "        newdic[sig_dsid_toplot[key][\"cat\"]] += 1\n",
    "    print(\"#\"*31)\n",
    "    print(\"#### Signal categories ####\")\n",
    "    print(\"#\"*31)\n",
    "    print(\"%-20s %10s\"%(\"Category\",\"N(samples)\"))\n",
    "    print(\"-\"*31)\n",
    "    for key in sorted(newdic.keys()):\n",
    "        print(\"%-20s %10s\"%(key,newdic[key]))\n",
    "    return sorted(newdic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBkgCategories():\n",
    "    newdic = {}\n",
    "    for key in bkg_dsid_toplot.keys():\n",
    "        if not bkg_dsid_toplot[key][\"cat\"] in newdic.keys():\n",
    "            newdic[bkg_dsid_toplot[key][\"cat\"]] = 0\n",
    "        newdic[bkg_dsid_toplot[key][\"cat\"]] += 1\n",
    "    print(\"#\"*31)\n",
    "    print(\"#### Background categories ####\")\n",
    "    print(\"#\"*31)\n",
    "    print(\"%-20s %10s\"%(\"Category\",\"N(samples)\"))\n",
    "    print(\"-\"*31)\n",
    "    for key in sorted(newdic.keys()):\n",
    "        print(\"%-20s %10s\"%(key,newdic[key]))\n",
    "    return sorted(newdic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSamplesInCategory(cat):\n",
    "    newdic = {}\n",
    "    for key in bkg_dsid_toplot.keys():\n",
    "        if bkg_dsid_toplot[key][\"cat\"] == cat:\n",
    "            newdic[bkg_dsid_toplot[key][\"DSID\"]] = key\n",
    "    for key in sig_dsid_toplot.keys():\n",
    "        if sig_dsid_toplot[key][\"cat\"] == cat:\n",
    "            newdic[sig_dsid_toplot[key][\"DSID\"]] = key\n",
    "    print(\"#\"*31)\n",
    "    print(\"####### Category %s #######\" %cat)\n",
    "    print(\"#\"*31)\n",
    "    print(\"%-20s %10s\"%(\"DSID\",\"Description\"))\n",
    "    print(\"-\"*31)\n",
    "    for key in sorted(newdic.keys()):\n",
    "        print(\"%-20s %10s\"%(key,newdic[key]))\n",
    "    return newdic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMCCategory(filedic):\n",
    "    MCcat = {}\n",
    "    for cat in filedic:\n",
    "        for dsid in filedic[cat][\"dsid\"]:\n",
    "            MCcat[dsid] = cat\n",
    "    return MCcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Backgrounds = getBkgCategories();\n",
    "Signals     = getSignalCategories();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSkims(folder):\n",
    "    onlyfiles = [f for f in listdir(folder) if isdir(join(folder, f))]\n",
    "    for of in onlyfiles:\n",
    "        print(of)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdic[\"lep_type\"] = {}\n",
    "plotdic[\"lep_type\"][\"nbin\"] = 2\n",
    "plotdic[\"lep_type\"][\"nmax\"] = 2\n",
    "plotdic[\"lep_type\"][\"nmin\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdic[\"lep_charge\"] = {}\n",
    "plotdic[\"lep_charge\"][\"nbin\"] = 3\n",
    "plotdic[\"lep_charge\"][\"nmax\"] = 1\n",
    "plotdic[\"lep_charge\"][\"nmin\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdic[\"lep_pt\"] = {}\n",
    "plotdic[\"lep_pt\"][\"nbin\"] = 100\n",
    "plotdic[\"lep_pt\"][\"nmax\"] = 1000\n",
    "plotdic[\"lep_pt\"][\"nmin\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdic[\"jet_pt\"] = {}\n",
    "plotdic[\"jet_pt\"][\"nbin\"] = 100\n",
    "plotdic[\"jet_pt\"][\"nmax\"] = 1000\n",
    "plotdic[\"jet_pt\"][\"nmin\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdic[\"met_et\"] = {}\n",
    "plotdic[\"met_et\"][\"nbin\"] = 100\n",
    "plotdic[\"met_et\"][\"nmax\"] = 1000\n",
    "plotdic[\"met_et\"][\"nmin\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdic[\"lep_eta\"] = {}\n",
    "plotdic[\"lep_eta\"][\"nbin\"] = 80\n",
    "plotdic[\"lep_eta\"][\"nmax\"] = 4\n",
    "plotdic[\"lep_eta\"][\"nmin\"] = -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdic[\"lep_phi\"] = {}\n",
    "plotdic[\"lep_phi\"][\"nbin\"] = 80\n",
    "plotdic[\"lep_phi\"][\"nmax\"] = 4\n",
    "plotdic[\"lep_phi\"][\"nmin\"] = -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdic[\"lep_E\"] = {}\n",
    "plotdic[\"lep_E\"][\"nbin\"] = 100\n",
    "plotdic[\"lep_E\"][\"nmax\"] = 1000\n",
    "plotdic[\"lep_E\"][\"nmin\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdic[\"lep_trackd0pvunbiased\"] = {}\n",
    "plotdic[\"lep_trackd0pvunbiased\"][\"nbin\"] = 200\n",
    "plotdic[\"lep_trackd0pvunbiased\"][\"nmax\"] = 10\n",
    "plotdic[\"lep_trackd0pvunbiased\"][\"nmin\"] = -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdic[\"lep_z0\"] = {}\n",
    "plotdic[\"lep_z0\"][\"nbin\"] = 200\n",
    "plotdic[\"lep_z0\"][\"nmax\"] = 10\n",
    "plotdic[\"lep_z0\"][\"nmin\"] = -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdic[\"lep_z0SinTheta\"] = {}\n",
    "plotdic[\"lep_z0SinTheta\"][\"nbin\"] = 100\n",
    "plotdic[\"lep_z0SinTheta\"][\"nmax\"] = 10\n",
    "plotdic[\"lep_z0SinTheta\"][\"nmin\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdic[\"mll\"] = {}\n",
    "plotdic[\"mll\"][\"nbin\"] = 200\n",
    "plotdic[\"mll\"][\"nmax\"] = 2000\n",
    "plotdic[\"mll\"][\"nmin\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdic[\"mt2\"] = {}\n",
    "plotdic[\"mt2\"][\"nbin\"] = 200\n",
    "plotdic[\"mt2\"][\"nmax\"] = 2000\n",
    "plotdic[\"mt2\"][\"nmin\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdic[\"costhstar\"] = {}\n",
    "plotdic[\"costhstar\"][\"nbin\"] = 10\n",
    "plotdic[\"costhstar\"][\"nmax\"] = 1\n",
    "plotdic[\"costhstar\"][\"nmin\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_plot_dic = {}\n",
    "colors = [(208, 240, 193), (195, 138, 145), (155, 152, 204), (248, 206, 104), \n",
    "          (222, 90, 106), (182, 70, 45), (153, 70, 15), (265, 17, 24), (245, 29, 256)]\n",
    "i = 0\n",
    "for b in Backgrounds:\n",
    "    bkg_plot_dic[b] = {\"color\":colors[i]}\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the scale factor needed for the scaling of MC to the right luminosity. In stead of storing all the individual scale factors, weights and cross-sections this function calculates the final scale factor so that the individual branches (i.e. scaleFactor_*, mc_weight, etc.) do not need to be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sf(xsec,lumi,nev,mcWeight,scaleFactor_PILEUP,scaleFactor_ELE,scaleFactor_MUON,scaleFactor_BTAG,scaleFactor_LepTRIGGER):\n",
    "    if lumi <= 0: \n",
    "        print(\"Lumi {:d} is not valid\".format(lumi)) \n",
    "        return 0\n",
    "    wgt = (mcWeight)*(scaleFactor_PILEUP)*(scaleFactor_ELE)*(scaleFactor_MUON)*(scaleFactor_BTAG)*(scaleFactor_LepTRIGGER)\n",
    "    return wgt * ((xsec*lumi)/nev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follwoing function is not really used anymore, but maybe it will become useful at some point. So saves it here :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skimming<br>\n",
    "The following function specify the skimming. Due to memory consumption it is not feasible to read all events so some skimming is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skimming(df,events,prev_n,next_n,nlep,lep_ptcut):\n",
    "     \n",
    "    pt  = awkward.from_iter(df['lep_pt'])\n",
    "    met = awkward.from_iter(df['met_et'])\n",
    "    \n",
    "    skim = []\n",
    "    \n",
    "    pt_org  = awkward.from_iter(df['lep_pt'])\n",
    "    \n",
    "    # MET > 50 GeV\n",
    "    #skim.append(pd.Series(met > 50000,name='bools').astype(int))\n",
    "    \n",
    "    # Makes sure that each sub-vector has the same number of entries (filling -999 if not)\n",
    "    awkward.pad_none(pt,nlep)#.fillna(-999)\n",
    "    awkward.pad_none(pt_org,nlep)#.fillna(-999)\n",
    "    \n",
    "    # Require leptons to have enough pt (according to values set in lep_ptcut)\n",
    "    for i in range(len(lep_ptcut)):\n",
    "        se = pt > lep_ptcut[i]\n",
    "        skim.append(pd.Series(awkward.count(pt[pt > lep_ptcut[i]],axis=1) >= 1).astype(int))\n",
    "        mask = np.logical_and(pt != pt.max(), pt.max != None)\n",
    "        pt = pt[mask]\n",
    "        \n",
    "     \n",
    "    # Make sure we only have exactly nlep (after the pt cuts)\n",
    "    skim.append(pd.Series(awkward.count(pt_org[pt_org != None],axis=1) == nlep).astype(int))\n",
    "        \n",
    "    # < here one can add additional cuts. Remeber to append the result to the skim vector)\n",
    "    \n",
    "    # Make sure that all our entries in the skim vector has value 1 \n",
    "    # If not this means that one of the cuts above did not pass (i.e. we don't want to keep the event)\n",
    "    # Adding values from all skim vectors together should give a total equal to the length of the skim vector\n",
    "    sk_final = skim[0]\n",
    "    for i in range(1,len(skim)):\n",
    "        sk_final = sk_final.add(skim[i])\n",
    "    final_skim = pd.Series(sk_final == len(skim))\n",
    "    \n",
    "    # Keep only rows where we have right number of leptons with pT above thresholds\n",
    "    df = df[final_skim.values]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation<br>\n",
    "The following function let you add new jet variables into the panda data frame. The jet information is removed apriori as the exact number of jets is varying from event to event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jetaugmentation(df,events,prev_n,next_n):\n",
    "    #pt, eta, phi, e = events.arrays(['jet_pt','jet_eta','jet_phi','jet_E'],outputtype=tuple,entrystart=prev_n,entrystop=next_n)\n",
    "    pt  = awkward.from_iter(df['jet_pt'])\n",
    "    eta = awkward.from_iter(df['jet_eta'])\n",
    "    phi = awkward.from_iter(df['jet_phi'])\n",
    "    e   = awkward.from_iter(df['jet_E'])\n",
    "    df['jet_n60'] = awkward.count(pt[pt > 60000],axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function let you add lepton variables. The jagged arrays need to be turned into variables for each lepton. If you apply skimming on 2 leptons the information of the two hardest leptons will be saved. If skimming is set to 3 it saves the 3 hardest leptons etc. One can also add higher level variables like mll, deltaR etc. See documentation here: [here](https://github.com/scikit-hep/uproot#multiple-values-per-event-jagged-arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lepaugmentation(df,events,prev_n,next_n,nlep):\n",
    "    pt_org  = awkward.from_iter(df['lep_pt'])\n",
    "    pt  = awkward.from_iter(df['lep_pt'])\n",
    "    eta = awkward.from_iter(df['lep_eta'])\n",
    "    phi = awkward.from_iter(df['lep_phi'])\n",
    "    e   = awkward.from_iter(df['lep_E'])\n",
    "  \n",
    "    pt  = awkward.pad_none(pt, nlep,axis=1)#.fillna(-999)pt.pad(nlep).fillna(-999)\n",
    "    eta = awkward.pad_none(eta,nlep,axis=1)#.fillna(-999)\n",
    "    phi = awkward.pad_none(phi,nlep,axis=1)#.fillna(-999)\n",
    "    e   = awkward.pad_none(e,  nlep,axis=1)#.fillna(-999)\n",
    "    \n",
    "    \n",
    "    pt_org  = awkward.pad_none(pt, nlep,axis=1)#.fillna(-999)pt.pad(nlep).fillna(-999)\n",
    "    # Make the lepton variables\n",
    "    for i in range(1,nlep+1):\n",
    "        mask = np.logical_and(pt == awkward.max(pt,axis=1), awkward.max(pt,axis=1) != None)\n",
    "        \n",
    "        if awkward.sum(np.logical_and(awkward.sum(mask,axis=1) != 1, True)) > 0:\n",
    "            idx = awkward.argmax(np.logical_and(awkward.sum(mask,axis=1) != 1, True))\n",
    "            print(\"There's a problem with idx %i in pt array %s\"%(idx,pt[idx]))\n",
    "            for i in range(0,len(pt[idx])):\n",
    "                print(\"pt = %f, eta = %f, E = %f\"%(pt[idx][i],eta[idx][i],e[idx][i]))\n",
    "                vec = vector.obj(pt=pt[idx][i],eta=eta[idx][i],phi=phi[idx][i],e=e[idx][i])\n",
    "                print(\"x = %f, y = %f, z = %f, pt = %f\"%(vec.x,vec.y,vec.z,vec.rho))\n",
    "            print(\"Take the one with highest energy\")\n",
    "            mask = np.logical_and(e == awkward.max(e,axis=1), awkward.max(e,axis=1) != None)\n",
    "            print(\"mask = \",mask[idx])\n",
    "            if awkward.sum(np.logical_and(awkward.sum(mask,axis=1) != 1, True)) > 0:\n",
    "                idx = awkward.argmax(np.logical_and(awkward.sum(mask,axis=1) != 1, True))\n",
    "                print(\"There's still a problem with idx %i in e array %s\"%(idx,e[idx]))\n",
    "               \n",
    "        df['lep%i_pt'%i]  = awkward.to_numpy(pt[mask])\n",
    "        df['lep%i_eta'%i] = awkward.to_numpy(eta[mask])\n",
    "        df['lep%i_phi'%i] = awkward.to_numpy(phi[mask])\n",
    "        df['lep%i_E'%i]   = awkward.to_numpy(e[mask])\n",
    "        \n",
    "        pt  = pt[~mask]\n",
    "        eta = eta[~mask]\n",
    "        phi = phi[~mask]\n",
    "        e   = e[~mask]\n",
    "\n",
    "    tlv = []\n",
    "    for i in range(1,nlep+1):\n",
    "        tlv.append(vector.array({\"pt\":df['lep%i_pt'%i].to_list(),\"eta\":df['lep%i_eta'%i].to_list(),\"phi\":df['lep%i_phi'%i].to_list(),\"e\":df['lep%i_E'%i].to_list()}))\n",
    "\n",
    "    pairs = awkward.argcombinations(pt_org,2)\n",
    "    \n",
    "    for ilep in range(len(pairs[0])):\n",
    "        i = pairs[0][ilep]['0']\n",
    "        j = pairs[0][ilep]['1']\n",
    "        \n",
    "        df['mll_%i%i'%(i+1,j+1)]   = (tlv[i]+tlv[j]).mass\n",
    "        df['dphi_%i%i'%(i+1,j+1)] = tlv[i].deltaphi(tlv[j])\n",
    "        df['dR_%i%i'%(i+1,j+1)]   = tlv[i].deltaR(tlv[j])\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataFrames(indir,nlep,chunksize,printevry,datatype,skimtag,lep_ptcut,branch_selection,lumi,categories = [], vb = 0):\n",
    "    # Count how many events and files we write/read\n",
    "    n_allfiles   = 0\n",
    "    nfile   = 0\n",
    "    totev   = 0\n",
    "    totev_skim = 0\n",
    "    out_filenum = 1\n",
    "    #try:\n",
    "    #    del [result]\n",
    "    #except:\n",
    "    #    print(\"WARNING\\t Result does not exists. Good :-)\")\n",
    "           \n",
    "    root_files = initialize(indir)\n",
    "    \n",
    "    dir_path = os.path.dirname(os.path.realpath('__file__'))\n",
    "    \n",
    "    datatypedir = dir_path+\"/\"+datatype+\"/\"\n",
    "    if not os.path.isdir(datatypedir):\n",
    "        os.makedirs(datatypedir)\n",
    "        print(\"INFO Created folder : \", datatypedir)\n",
    "    hdf5dir = datatypedir+\"/hdf5/\"\n",
    "    if not os.path.isdir(hdf5dir):\n",
    "        os.makedirs(hdf5dir)\n",
    "        print(\"INFO \\t Dreated folder : \", hdf5dir)\n",
    "    else:\n",
    "        print(\"WARNING \\t Output folder %s already exists\" %hdf5dir)\n",
    " \n",
    "    # Checking if file exitst (not needed)\n",
    "    #path = indir+\"/%s_%s.h5\" %(datatype,skimtag)\n",
    "    #if os.path.exists(path):\n",
    "    #    os.remove(path)\n",
    "    #    print(\"WARNING\\t Removing file {:s}\".format(path))\n",
    "    for cat in root_files.keys():\n",
    "        \n",
    "        if os.path.isfile(hdf5dir+\"/%s_%s_num.h5\" %(cat,skimtag)):\n",
    "            print(\"INFO \\t Skipping %s since file %s already exists\"%(cat,hdf5dir+\"/%s_%s_num.h5\" %(cat,skimtag)))\n",
    "            continue\n",
    "        \n",
    "        if len(categories) and not cat in categories:\n",
    "            print(\"INFO \\t Skipping category %s\"%cat)\n",
    "            continue\n",
    "        \n",
    "        isSignal = False\n",
    "        isBkg = False\n",
    "        isData = False\n",
    "        if root_files[cat]['type'] == 'signal':\n",
    "            isSignal = True\n",
    "        elif root_files[cat]['type'] == 'bkg':\n",
    "            isBkg = True\n",
    "        elif root_files[cat]['type'] == 'data':\n",
    "            isData = True\n",
    "        else:\n",
    "            print(\"ERROR \\t Could not find type of data set for category {:s}. Skipping\".format(cat))\n",
    "        print(\"INFO \\t Category %s is %s\"%(cat,\"signal\" if isSignal else (\"background\" if isBkg else \"data\")))\n",
    "        \n",
    "        nfile = 0\n",
    "        n_allfiles = 0\n",
    "        for f in root_files[cat][\"files\"]:\n",
    "            #if not \"410012\" in f: continue\n",
    "            n_allfiles += 1\n",
    "            # Getting the file and extracting the information from info dictionary\n",
    "            \n",
    "            events = uproot.open(f+\":mini\")\n",
    "            \n",
    "            nentries = events.num_entries\n",
    "            \n",
    "            print(\"INFO  \\t Opening file {:d}/{:d} for category {:s}: {:s} with {:d} events\".format(n_allfiles,\n",
    "                                                                                      len(root_files[cat][\"files\"]), cat,\n",
    "                                                                                      f.split(\"/\")[-1],\n",
    "                                                                                      nentries))\n",
    "            \n",
    "            path = indir+\"/%s_%s_num_%i.h5\" %(datatype,skimtag,out_filenum)\n",
    "            \n",
    "            #branches = events.keys()\n",
    "            \n",
    "            \n",
    "            n = 19\n",
    "            prev_n = 18000000\n",
    "            while True:\n",
    "                # Measure time to read \n",
    "                if n == 19: start = time.time()   \n",
    "                else:   \n",
    "                    end = time.time()\n",
    "                    dur = (end - start)\n",
    "                    dur_sec = chunksize/dur\n",
    "                    m, s = divmod((nentries-((n-1)*chunksize))/dur_sec, 60)\n",
    "                    h, m = divmod(m, 60)\n",
    "                    print(\"INFO  \\t Event/sec  = {:.0f}. ETC = {:d}h{:02d}m{:02d}s\".format(dur_sec,int(h),int(m),int(s)))\n",
    "                    start = time.time()\n",
    "                # Get the range of events to read\n",
    "                next_n = n*chunksize if n*chunksize < nentries else nentries\n",
    "                print(\"INFO  \\t Reading entries {:d} - {:d} of {:d}. Total so far: {:d}\".format(prev_n,next_n,nentries,totev_skim))\n",
    "                #df = events.arrays(branches)#flatten=False,entrystart=prev_n,entrystop=next_n)\n",
    "                #df = events.pandas.df(branches,flatten=False,entrystart=prev_n,entrystop=next_n)\n",
    "                np_array = events.arrays(filter_name=branch_selection,entry_start=prev_n,\n",
    "                                         entry_stop=next_n,library=\"np\")\n",
    "                df = pd.DataFrame(np_array)\n",
    "                \n",
    "                #print(\"df.columns = \",df.columns)\n",
    "                \n",
    "                df = df.astype({'channelNumber': 'float32'})     \n",
    "                df = df.astype({'eventNumber': 'float32'})\n",
    "                totev += len(df.index)\n",
    "                if df.shape[0] > 0:\n",
    "                    # If MC: get the scale factor corresponding to the total data luminosity (is set above). \n",
    "                    # If data: set scale factor to 1!\n",
    "                    if not \"data\" in f:\n",
    "                        df['wgt'] = np.vectorize(calc_sf)(df.XSection,lumi,df.SumWeights,df.mcWeight,\n",
    "                                                         df.scaleFactor_PILEUP,df.scaleFactor_ELE,\n",
    "                                                         df.scaleFactor_MUON,df.scaleFactor_BTAG,\n",
    "                                                         df.scaleFactor_LepTRIGGER)\n",
    "                    else:\n",
    "                        df['wgt'] = 1.0\n",
    "                    # Define if it is signal or background (important for ML classification)\n",
    "                    if isBkg or isData:\n",
    "                        df['isSignal'] = 0\n",
    "                    elif isSignal:\n",
    "                        df['isSignal'] = 1\n",
    "                    else:\n",
    "                        df['isSignal'] = 0\n",
    "                    df['MCType'] = cat\n",
    "                    #df = df.astype({'MCType': 'string'})\n",
    "                    ###########   \n",
    "                    # Skimming\n",
    "                    ###########  \n",
    "                    if vb > 4: print(\"DEBUG \\t Skimming dataframe with shape \",df.shape)\n",
    "                    df = skimming(df,events,prev_n,next_n,nlep,lep_ptcut)\n",
    "                    if vb > 4: print(\"DEBUG \\t Shape after skimming is \",df.shape)\n",
    "                if df.shape[0] > 0: \n",
    "                    ##############\n",
    "                    # Augumenting\n",
    "                    ##############\n",
    "                    if vb > 4: print(\"DEBUG \\t Augumenting dataframe with shape \",df.shape)\n",
    "                    df = jetaugmentation(df,events,prev_n,next_n)\n",
    "                    df = lepaugmentation(df,events,prev_n,next_n,nlep)\n",
    "                    if vb > 4: print(\"DEBUG \\t Done augumenting dataframe, now is shape \",df.shape)\n",
    "                    ###########   \n",
    "                    # Slimming\n",
    "                    ###########\n",
    "                    if vb > 4: print(\"DEBUG \\t Slimming dataframe with shape \",df.shape)\n",
    "                    df = df.drop(['mcWeight','scaleFactor_PILEUP','scaleFactor_ELE','scaleFactor_MUON','scaleFactor_BTAG','scaleFactor_LepTRIGGER'],axis=1)\n",
    "                    df = df.drop(['jet_n', 'jet_pt', 'jet_eta', 'jet_phi', 'jet_E', 'jet_jvt'],axis=1)\n",
    "                    df = df.drop(['jet_trueflav', 'jet_truthMatched', 'jet_MV2c10', 'jet_pt_syst'],axis=1)\n",
    "                    df = df.drop(['lep_n', 'lep_truthMatched', 'lep_trigMatched', 'lep_pt', 'lep_eta'],axis=1)\n",
    "                    df = df.drop(['lep_phi', 'lep_E', 'lep_z0', 'lep_charge', 'lep_type', 'lep_isTightID'],axis=1)\n",
    "                    df = df.drop(['lep_ptcone30', 'lep_etcone20', 'lep_trackd0pvunbiased'],axis=1)\n",
    "                    df = df.drop(['lep_tracksigd0pvunbiased', 'lep_pt_syst'],axis=1)\n",
    "                    if vb > 4: print(\"DEBUG \\t Done slimming dataframe, now is shape \",df.shape)\n",
    "                    # If first time, create result data frame. If not; concatenate\n",
    "                    try: \n",
    "                        result = pd.concat([result,df],axis=0, ignore_index=True)\n",
    "                    except:\n",
    "                        result = df\n",
    "                        print(\"WARNING\\t Starting a new result panda\")\n",
    "                    totev_skim += len(df.index)\n",
    "                    \n",
    "                    # Delete the temporary data frame\n",
    "                    del [df]\n",
    "                if totev_skim > printevry:\n",
    "                    #result = sortAndIndex(result,nlep)\n",
    "                    path = hdf5dir+\"/%s_%s_num.h5\" %(cat,skimtag)\n",
    "                    #print(\"{10:s} {7:d} {2:d} {7:d} {7:d}\".format(cat,totev,nfile,totev_skim,\n",
    "                                                                  #(float(totev_skim)/float(totev))*100.))\n",
    "                    if totev:\n",
    "                        print(\"INFO  \\t <-Read {:d} events in {:d} files for category {:s}, for which {:d} ({:.2f}%) were written to \\n       \\t{:s}\"\n",
    "                            .format(totev,nfile,cat,totev_skim,(float(totev_skim)/float(totev))*100.,path))\n",
    "                    print(\"result = \",result.columns)\n",
    "                    result.to_hdf(path,'mini',mode='a',format='table')\n",
    "                    out_filenum += 1\n",
    "                    totev = 0\n",
    "                    totev_skim = 0\n",
    "                    #nfile = 0\n",
    "                    del [result]\n",
    "                # If read everything\n",
    "                if n*chunksize > nentries: break\n",
    "                # Update counters before continuing\n",
    "                prev_n = n*chunksize + 1\n",
    "                n += 1\n",
    "                #break\n",
    "            nfile += 1\n",
    "            del [events]  \n",
    "            #break\n",
    "        # Make sure we write the last events for this category\n",
    "        #result = sortAndIndex(result,nlep)\n",
    "        path = hdf5dir+\"/%s_%s_num.h5\" %(cat,skimtag)\n",
    "        try:\n",
    "            result.to_hdf(path,'mini',mode='a',format='table')\n",
    "            if totev:\n",
    "                print(\"INFO  \\t ->Read {:d} events in {:d} files for category {:s}, for which {:d} ({:.2f}%) were written to \\n       \\t{:s}\"\n",
    "                        .format(totev,nfile,cat,totev_skim,(float(totev_skim)/float(totev))*100.,path))\n",
    "        except:\n",
    "            print(\"INFO \\t Everything already written to file.\")\n",
    "        totev = 0\n",
    "        totev_skim = 0\n",
    "        try:\n",
    "            del [result]\n",
    "        except:\n",
    "            print(\"WARNING \\t Result does not exist. Probably empty file.\")\n",
    "    \n",
    "    #return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#branch_selection = \"/(met_|jet_|lep_|scaleFactor_|eventNumber|channelNumber|XSection|SumWeights|mcWeight)/\" \n",
    "#createDataFrames(\"/storage/shared/data/fys5555/ATLAS_opendata/4lep/MC/\",4, 50000, 50000, \"MC\", \"4L\", [], branch_selection, 10.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertRDFCutflowToTex(cutflow):\n",
    "    i = 0\n",
    "    tabstr = \"\"\n",
    "    for c in cutflow1:\n",
    "        cname = c.GetName()\n",
    "        c2 = cutflow2.At(cname)\n",
    "        if i == 0:\n",
    "            nevc1 = c.GetAll()\n",
    "            nevc2 = c2.GetAll()\n",
    "        cname = cname.replace(\">\",\"$>$\")\n",
    "        cname = cname.replace(\"<\",\"$<$\")\n",
    "        tabstr += \"%-30s & $%.0f$ & $%.0f$ & $%.2f$ & $%.2f$ & $%.0f$ & $%.0f$ & $%.2f$ & $%.2f$ \\\\\\ \\n\"%(cname,c.GetPass(),c.GetAll(),c.GetEff(),(c.GetPass()/nevc1)*100.,c2.GetPass(),c2.GetAll(),c2.GetEff(),(c2.GetPass()/nevc2)*100.)\n",
    "        i += 1\n",
    "    print(tabstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
