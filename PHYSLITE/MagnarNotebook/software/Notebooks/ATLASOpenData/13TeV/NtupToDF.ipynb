{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When running this notebook via the Galaxy portal\n",
    "You can access your data via the dataset number. Using a Python kernel, you can access dataset number 42 with ``handle = open(get(42), 'r')``.\n",
    "To save data, write your data to a file, and then call ``put('filename.txt')``. The dataset will then be available in your galaxy history.\n",
    "<br><br>Note that if you are putting/getting to/from a different history than your default history, you must also provide the history-id.\n",
    "<br><br>More information including available galaxy-related environment variables can be found at https://github.com/bgruening/docker-jupyter-notebook. This notebook is running in a docker container based on the Docker Jupyter container described in that link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ntuple to data frame conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook converts ntuples to pandas data frame and writes the output to hdf5 files. Events are selected when running over the ntuples and new variables are created and put into a data frame. Code adds the background category, whether the event is coming from a signal simulation or not (useful when training a BDT or NN) and the weight used to scale the MC to data.\n",
    "\n",
    "The current code takes about 4 - 5 hours on the simulated 2Lep background and signal samples. I.e. processing about 118 million events.\n",
    "\n",
    "First import some of the needed modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT as R\n",
    "import import_ipynb\n",
    "import setPath\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from Input.OpenDataPandaFramework13TeV import *\n",
    "%jsroot on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the path to the open data ntuples and which skim you are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R.gSystem.Load(\"/storage/shared/software/Input/CalcGenericMT2/src/libBinnedLik.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opendatadir = \"/storage/shared/data/fys5555/ATLAS_opendata/\"\n",
    "analysis = \"2lep\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the ROOT::TChain for adding all the root files and eventually looping over all the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = R.TChain(\"mini\")\n",
    "data = R.TChain(\"mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the MC and data files available for the selected data set and make lists with the background and signal categories (useful information to add into the data frame later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcfiles = initialize(opendatadir+\"/\"+analysis+\"/MC\")\n",
    "datafiles = initialize(opendatadir+\"/\"+analysis+\"/Data\")\n",
    "allfiles = z = {**mcfiles, **datafiles}\n",
    "Backgrounds = getBkgCategories(); \n",
    "Signals = getSignalCategories();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more preparatory steps to classify the individual backgrounds into categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSignalCategories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCcat = {}\n",
    "for cat in allfiles:\n",
    "    for dsid in allfiles[cat][\"dsid\"]:\n",
    "        try:\n",
    "            MCcat[int(dsid)] = cat\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the background to the TChain and check number of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_IDs = []\n",
    "background.Reset()\n",
    "for b in Backgrounds+Signals:\n",
    "    i = 0\n",
    "    if not b in mcfiles.keys(): continue\n",
    "    for mc in mcfiles[b][\"files\"]:\n",
    "        if not os.path.isfile(mc): continue\n",
    "        try:\n",
    "            dataset_IDs.append(int(mcfiles[b][\"dsid\"][i]))\n",
    "            background.Add(mc)\n",
    "        except:\n",
    "            print(\"Could not get DSID for %s. Skipping\"%mc)\n",
    "        i += 1\n",
    "nen = background.GetEntries()\n",
    "print(\"Added %i entries for backgrounds and signals\"%(nen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding all the available data into the TChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Reset(); \n",
    "for d in datafiles[\"data\"][\"files\"]:  \n",
    "    if not os.path.isfile(d): continue\n",
    "    data.Add(d)\n",
    "nen = data.GetEntries()\n",
    "print(\"Added %i entries for data\"%(nen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the variables/features we want to add to our data frame and which will be filled during the loop over events. Here you can add and remove variables depending on what you will use the resulting data frame to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {\"lep_pt1\":[],\"lep_eta1\":[],\"lep_phi1\":[],\"lep_E1\":[],\"lep_flav1\":[],\n",
    "           \"lep_pt2\":[],\"lep_eta2\":[],\"lep_phi2\":[],\"lep_E2\":[],\"lep_flav2\":[],\n",
    "           \"met\":[], \"mll\":[], \"njet20\":[], \"njet60\":[], \"nbjet60\":[],\"nbjet70\":[],\n",
    "           \"nbjet77\":[],\"nbjet85\":[],\"mt2_80\":[],\"mt2_0\":[],\n",
    "           \"isSF\":[], \"isOS\":[], \"weight\":[],\"category\":[],\"isSignal\":[],\n",
    "           \"lep_z01\":[], \"lep_z02\":[], \"lep_trackd0pvunbiased1\":[],\n",
    "           \"lep_trackd0pvunbiased2\":[], \"lep_tracksigd0pvunbiased1\":[], \"lep_tracksigd0pvunbiased2\":[],\n",
    "           \"lep_etcone201\":[],\"lep_etcone202\":[], \"lep_ptcone301\":[], \"lep_ptcone302\":[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the event loop (needs to be run twice; one for MC and one for data if you are interested in both). It makes some selections, creates new variables and fill the list in the dictionary defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "isData = 0; \n",
    "\n",
    "if isData == 1: ds = data \n",
    "else: ds = background     \n",
    "\n",
    "l1 = R.TLorentzVector() \n",
    "l2 = R.TLorentzVector() \n",
    "met = R.TLorentzVector() \n",
    "dileptons = R.TLorentzVector() \n",
    "    \n",
    "i = 0   \n",
    "for event in ds: \n",
    "    \n",
    "    if i%100000 == 0 and i>0: \n",
    "        print(\"Total events %i/%i\"%(i,ds.GetEntries()))\n",
    "        #break\n",
    "    i += 1 \n",
    "    \n",
    "    sig_lep_idx = []\n",
    "    nsig_lep = 0\n",
    "    for j in range(ds.lep_n):\n",
    "        if ds.lep_etcone20[j]/ds.lep_pt[j] > 0.15: continue\n",
    "        if ds.lep_ptcone30[j]/ds.lep_pt[j] > 0.15: continue\n",
    "        sig_lep_idx.append(j)\n",
    "        nsig_lep += 1\n",
    "        \n",
    "    if not nsig_lep == 2: continue \n",
    "    njet20 = 0\n",
    "    njet60 = 0\n",
    "    nbjet60 = 0\n",
    "    nbjet70 = 0\n",
    "    nbjet77 = 0\n",
    "    nbjet85 = 0\n",
    "    for j in range(ds.jet_n):\n",
    "        if ds.jet_pt[j] > 20000:\n",
    "            njet20 += 1\n",
    "            if ds.jet_MV2c10[j] > 0.9349:\n",
    "                nbjet60 += 1\n",
    "            if ds.jet_MV2c10[j] > 0.8244:\n",
    "                nbjet70 += 1\n",
    "            if ds.jet_MV2c10[j] > 0.6459:\n",
    "                nbjet77 += 1\n",
    "            if ds.jet_MV2c10[j] > 0.1758:\n",
    "                nbjet85 += 1\n",
    "        if ds.jet_pt[j] > 60000:\n",
    "            njet60 += 1\n",
    "        \n",
    "    ## Require \"good leptons\": \n",
    "    idx1 = sig_lep_idx[0]\n",
    "    idx2 = sig_lep_idx[1]\n",
    "    \n",
    "    ## Set Lorentz vectors: \n",
    "    l1.SetPtEtaPhiE(ds.lep_pt[idx1]/1000., ds.lep_eta[idx1], ds.lep_phi[idx1], ds.lep_E[idx1]/1000.);\n",
    "    l2.SetPtEtaPhiE(ds.lep_pt[idx2]/1000., ds.lep_eta[idx2], ds.lep_phi[idx2], ds.lep_E[idx2]/1000.);\n",
    "    \n",
    "    met.SetPtEtaPhiE(ds.met_et/1000., 0.0, ds.met_phi, 0.0);\n",
    "    \n",
    "    ## Variables are stored in the TTree with unit MeV, so we need to divide by 1000 \n",
    "    ## to get GeV, which is a more practical and commonly used unit. \n",
    "    \n",
    "    dileptons = l1 + l2;   \n",
    "    \n",
    "    # The stransverse mass!\n",
    "    mycalc_80 = R.ComputeMT2(l1,l2,met,0.,80.)\n",
    "    mycalc_0 = R.ComputeMT2(l1,l2,met,0.,0.)\n",
    "    \n",
    "    columns[\"lep_pt1\"].append(ds.lep_pt[idx1]/1000.0)\n",
    "    columns[\"lep_eta1\"].append(ds.lep_eta[idx1])\n",
    "    columns[\"lep_phi1\"].append(ds.lep_phi[idx1])\n",
    "    columns[\"lep_E1\"].append(ds.lep_E[idx1]/1000.0)\n",
    "    columns[\"lep_flav1\"].append(ds.lep_charge[idx1]*ds.lep_type[idx1])\n",
    "    columns[\"lep_z01\"].append(ds.lep_z0[idx1])\n",
    "    columns[\"lep_tracksigd0pvunbiased1\"].append(ds.lep_tracksigd0pvunbiased[idx1])\n",
    "    columns[\"lep_trackd0pvunbiased1\"].append(ds.lep_trackd0pvunbiased[idx1])\n",
    "    columns[\"lep_etcone201\"].append(ds.lep_etcone20[idx1])\n",
    "    columns[\"lep_ptcone301\"].append(ds.lep_ptcone30[idx1])\n",
    "    \n",
    "    columns[\"lep_pt2\"].append(ds.lep_pt[idx2]/1000.0)\n",
    "    columns[\"lep_eta2\"].append(ds.lep_eta[idx2])\n",
    "    columns[\"lep_phi2\"].append(ds.lep_phi[idx2])\n",
    "    columns[\"lep_E2\"].append(ds.lep_E[idx2]/1000.0)\n",
    "    columns[\"lep_flav2\"].append(ds.lep_charge[idx2]*ds.lep_type[idx2])\n",
    "    columns[\"lep_z02\"].append(ds.lep_z0[idx2])\n",
    "    columns[\"lep_tracksigd0pvunbiased2\"].append(ds.lep_tracksigd0pvunbiased[idx2])\n",
    "    columns[\"lep_trackd0pvunbiased2\"].append(ds.lep_trackd0pvunbiased[idx2])\n",
    "    columns[\"lep_etcone202\"].append(ds.lep_etcone20[idx2])\n",
    "    columns[\"lep_ptcone302\"].append(ds.lep_ptcone30[idx2])\n",
    "    \n",
    "    columns[\"mt2_80\"].append(mycalc_80.Compute())\n",
    "    columns[\"mt2_0\"].append(mycalc_0.Compute())\n",
    "    columns[\"met\"].append(ds.met_et/1000.0)\n",
    "    columns[\"mll\"].append(dileptons.M())\n",
    "    \n",
    "    columns[\"njet20\"].append(njet20)\n",
    "    columns[\"njet60\"].append(njet60)\n",
    "    \n",
    "    columns[\"nbjet60\"].append(nbjet60)\n",
    "    columns[\"nbjet70\"].append(nbjet70)\n",
    "    columns[\"nbjet77\"].append(nbjet77)\n",
    "    columns[\"nbjet85\"].append(nbjet85)\n",
    "    \n",
    "    Type = \"\"\n",
    "    if not isData:\n",
    "        Type = MCcat[ds.channelNumber]\n",
    "        # print(\"Type\",Type)\n",
    "        columns[\"category\"].append(Type)\n",
    "    else:\n",
    "        columns[\"category\"].append(\"data\")\n",
    "    \n",
    "    if Type in Backgrounds:\n",
    "        columns[\"isSignal\"].append(0)\n",
    "    elif Type in Signals:\n",
    "        columns[\"isSignal\"].append(1)\n",
    "    else:\n",
    "        columns[\"isSignal\"].append(0)\n",
    "    \n",
    "    if ds.lep_charge[idx1] == ds.lep_charge[idx2]: columns[\"isOS\"].append(0)\n",
    "    else: columns[\"isOS\"].append(1)\n",
    "        \n",
    "    if ds.lep_type[idx1] == ds.lep_type[idx2]: columns[\"isSF\"].append(1)\n",
    "    else: columns[\"isSF\"].append(0)\n",
    "        \n",
    "    if isData:\n",
    "        columns[\"weight\"].append(1.0)\n",
    "    else:\n",
    "        W = ((ds.mcWeight)*(ds.scaleFactor_PILEUP)*\n",
    "             (ds.scaleFactor_ELE)*(ds.scaleFactor_MUON)*\n",
    "             (ds.scaleFactor_BTAG)*(ds.scaleFactor_LepTRIGGER))*((ds.XSection*lumi)/ds.SumWeights)\n",
    "        columns[\"weight\"].append(W)\n",
    "        \n",
    "print(\"Done!\")\n",
    "if isData == 0:\n",
    "    print(\"Remebered to run over data? No? Set data = 1 at the top and run again\")\n",
    "else:\n",
    "    print(\"Remebered to run over MC? No? Set data = 0 at the top and run again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally convert the dictionary to a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"category\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and write it to a file for later use. There are many more possibilites for file format. Have a look at the pandas documentation (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_hdf.html) for possibilites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in columns.keys():\n",
    "    print(c,len(columns[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_hdf(\"/storage/shared/data/2lep_df_forML_bkg_signal_inclusive.hdf5\",\"mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "df[df['met'] < 1000].plot(y='lep_tracksigd0pvunbiased2',kind='hist',logy=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['met'] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reread = pd.read_hdf(\"/storage/shared/data/2lep_df_forML_bkg_signal_inclusive.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reread.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
