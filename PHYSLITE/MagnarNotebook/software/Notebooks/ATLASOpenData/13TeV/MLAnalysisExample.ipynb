{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When running this notebook via the Galaxy portal\n",
    "You can access your data via the dataset number. Using a Python kernel, you can access dataset number 42 with ``handle = open(get(42), 'r')``.\n",
    "To save data, write your data to a file, and then call ``put('filename.txt')``. The dataset will then be available in your galaxy history.\n",
    "<br><br>Note that if you are putting/getting to/from a different history than your default history, you must also provide the history-id.\n",
    "<br><br>More information including available galaxy-related environment variables can be found at https://github.com/bgruening/docker-jupyter-notebook. This notebook is running in a docker container based on the Docker Jupyter container described in that link.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Machine Learning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook apply some examples of ML methods available in <a href=\"https://scikit-learn.org/stable/\" target=\"_blank\">scikit-learn</a>. The notbook reads in hdf5 files for testing and training from disk. If these are not available they can be produced using the following two notebooks:\n",
    "\n",
    "1. **ConvertNtupToHdf5** - convert openData ntuples to hdf5 files using uproot.\n",
    "2. **MakeTrainTestSamples** - divides the hdf5 files into training and test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To make the notebook view a bit wider\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some imports and includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from setPath.ipynb\n",
      "importing Jupyter notebook from /home/eirikgr/software/Input/OpenDataPandaFramework13TeV.ipynb\n",
      "This library contains handy functions to ease the access and use of the 13TeV ATLAS OpenData release\n",
      "\n",
      "getBkgCategories()\n",
      "\t Dumps the name of the various background cataegories available \n",
      "\t as well as the number of samples contained in each category.\n",
      "\t Returns a vector with the name of the categories\n",
      "\n",
      "getSamplesInCategory(cat)\n",
      "\t Dumps the name of the samples contained in a given category (cat)\n",
      "\t Returns dictionary with keys being DSIDs and values physics process name from filename.\n",
      "\n",
      "getMCCategory()\n",
      "\t Returns dictionary with keys DSID and values MC category\n",
      "\n",
      "initialize(indir)\n",
      "\t Collects all the root files available in a certain directory (indir)\n",
      "\n",
      "getSkims(indir)\n",
      "\t Prints all available skims in the directory\n",
      "\n",
      "\n",
      "\n",
      "Setting luminosity to 10064 pb^-1\n",
      "\n",
      "###############################\n",
      "#### Background categories ####\n",
      "###############################\n",
      "Category             N(samples)\n",
      "-------------------------------\n",
      "Diboson                      10\n",
      "Higgs                        20\n",
      "Wjets                        42\n",
      "Wjetsincl                     6\n",
      "Zjets                        42\n",
      "Zjetsincl                     3\n",
      "singleTop                     6\n",
      "topX                          3\n",
      "ttbar                         1\n",
      "###############################\n",
      "#### Signal categories ####\n",
      "###############################\n",
      "Category             N(samples)\n",
      "-------------------------------\n",
      "GG_ttn1                       4\n",
      "Gee                           5\n",
      "Gmumu                         5\n",
      "RS_G_ZZ                       5\n",
      "SUSYC1C1                     10\n",
      "SUSYC1N2                     18\n",
      "SUSYSlepSlep                 14\n",
      "TT_directTT                   4\n",
      "ZPrimeee                      4\n",
      "ZPrimemumu                    4\n",
      "ZPrimett                     12\n",
      "dmV_Zll                      10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%jsroot` not found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import import_ipynb\n",
    "import setPath\n",
    "from Input.OpenDataPandaFramework13TeV import * \n",
    "%jsroot on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbclassifier = xgb.XGBClassifier()\n",
    "xgbclassifier.load_model(\"mymodel.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for categorizing training and test into signal and background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorizeTrainandTest(samples, Signals, Backgrounds):\n",
    "    files_signal = []\n",
    "    files_background = []\n",
    "    for t in samples:\n",
    "        found = False\n",
    "        for s in Signals:\n",
    "            if s+\"_\" in t:\n",
    "                files_signal.append(t)\n",
    "                found = True\n",
    "                break\n",
    "        if found: continue\n",
    "        for b in Backgrounds:\n",
    "            if b+\"_\" in t:\n",
    "                print(b,t)\n",
    "                files_background.append(t)\n",
    "                found = True\n",
    "                break\n",
    "    return files_signal, files_background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifies the following\n",
    "* location of hdf5 files for MC (signal and background) and data.  \n",
    "* the skimtag (used when producing the hdf5 files)\n",
    "* the signal model/DSID you want to use to supervise the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcdir = \"/scratch/eirikgr/openData_13TeV/2lep/MC/hdf5/\"\n",
    "datadir = \"/scratch/eirikgr/openData_13TeV/2lep/Data/hdf5/\"\n",
    "\n",
    "Backgrounds = getBkgCategories();\n",
    "\n",
    "skimtag = \"2L_pt25_25_met50\"\n",
    "\n",
    "Backgrounds.remove('Wjetsincl')\n",
    "Backgrounds.remove('Zjetsincl')\n",
    "\n",
    "print(Backgrounds)\n",
    "\n",
    "Signals = [\"SUSYC1N2\"]\n",
    "# Set to specific DSID if only want to train on one sample, \n",
    "# if not train on all samples in signal model specified above\n",
    "signal_dsid = -1\n",
    "\n",
    "testing_files = [f for f in listdir(mcdir) if (f.endswith('.h5') and (f.startswith(\"testing\") and skimtag in f))]\n",
    "training_files = [f for f in listdir(mcdir) if (f.endswith('.h5') and (f.startswith(\"training\") and skimtag in f))]\n",
    "\n",
    "training_files_signal, training_files_background = categorizeTrainandTest(training_files,Signals,Backgrounds)\n",
    "testing_files_signal, testing_files_background = categorizeTrainandTest(testing_files,Signals,Backgrounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for getting the training and test data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTrainTestDF(f_signal,f_background,signal_dsid = -1):\n",
    "    sig = []\n",
    "    bkg = []\n",
    "    for tfs in f_signal:\n",
    "        df = pd.read_hdf(mcdir+\"/\"+tfs)\n",
    "        if signal_dsid > 0:\n",
    "            df = df.loc[df['channelNumber'] == signal_dsid]\n",
    "        sig.append(df)\n",
    "        print(\"ÌNFO \\t Adding %s to DF\"%tfs)\n",
    "    for tfb in f_background:\n",
    "        bkg.append(pd.read_hdf(mcdir+\"/\"+tfb))\n",
    "        print(\"ÌNFO \\t Adding %s to DF\"%tfb)\n",
    "    merged_train = pd.concat(sig + bkg)\n",
    "    print(\"\\n\")\n",
    "    return merged_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training and testing DFs\n",
    "merged_training = GetTrainTestDF(training_files_signal, training_files_background, signal_dsid)\n",
    "merged_testing = GetTrainTestDF(testing_files_signal, testing_files_background, signal_dsid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_training[\"mll_12\"]/1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing the variables to be dropped in the ML algorithm. \n",
    "#### Note the beauty of pandas: the variable names are entered, compared to numpy arrays which don't have this feature. You can add/remove as many variables as you wish to improve the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in merged_training.columns:\n",
    "    print(c)\n",
    "todrop = ['XSection','SumWeights','eventNumber','channelNumber',\"wgt\",'isSignal','MCType']\n",
    "X_train = merged_training.drop(todrop,axis = 1)\n",
    "Y_train = merged_training['isSignal']\n",
    "\n",
    "X_test = merged_training.drop(todrop,axis = 1)\n",
    "Y_test = merged_training['isSignal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At this point, you have to choose the [ML algorithm](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) (BDT, logistic regression, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBClassifier\n",
    "Let's have a look at the [XGBoost classifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BDT classifier\n",
    "xgbclassifier = xgb.XGBClassifier(\n",
    "    max_depth=3, \n",
    "    n_estimators=120,\n",
    "    learning_rate=0.1,\n",
    "    n_jobs=4,\n",
    "    #scale_pos_weight=sum_wbkg/sum_wsig,\n",
    "    objective='binary:logistic')\n",
    "    #missing=-999.0) \n",
    "xgbclassifier.fit(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As for any descision trees XGBoost lets you look at the variable importance and a long range of [other things](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.plotting) for your model. Like the feature importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot variable importance\n",
    "fig_size = plt.rcParams[\"figure.figsize\"] \n",
    "ax = xgb.plot_importance(xgbclassifier)\n",
    "ax.xaxis.label.set_size(20)\n",
    "ax.yaxis.label.set_size(30)\n",
    "fig_size[0] = 20\n",
    "fig_size[1] = 15\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "plt.show()\n",
    "y_pred = xgbclassifier.predict(X_test)\n",
    "y_pred_prob = xgbclassifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ... or the descision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_tree(xgbclassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell allow you to check the classification performance of your algorithm. The y-axis is the number of events and the x-axis is the probability that the sample is signal. The blue distribution corresponds to background and the pink to signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting not specific to any ML model\n",
    "\n",
    "Let's have a look at some other interesting plots which are not specific to any ML algorithm.\n",
    "\n",
    "#### First have a look at the distribution of the score from the classification algorithm using the unseen test data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#  histogram of the ML outputs\n",
    "n, bins, patches = plt.hist(y_pred_prob[:,1][Y_test==0], 100,  facecolor='blue', alpha=0.2,label=\"Background\")\n",
    "n, bins, patches = plt.hist(y_pred_prob[:,1][Y_test==1], 100,  facecolor='red', alpha=0.2, label=\"Signal\")\n",
    "plt.xlabel('ML output')\n",
    "plt.ylabel('Events')\n",
    "plt.yscale('log')\n",
    "plt.title('ML output, OpenData dataset, validation data')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In machine learning, the performance of the algorithms can be studied using different [metrics](https://scikit-learn.org/stable/modules/classes.html?highlight=sklearn%20metrics#module-sklearn.metrics) such as the Compute Receiver operating characteristic ([ROC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve)) curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,auc\n",
    "fpr, tpr, thresholds = roc_curve(Y_test,y_pred_prob[:,1], pos_label=1)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC on OpenData 13TeV dataset')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ROC curve is related to the [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix). The confusion matrix is plotted bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plot_confusion_matrix(Y_test, y_pred, ['background','signal'], normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One can also compare the predicted vs. true distributions of both background and signal for some given variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "h_true_bkg = []\n",
    "h_true_sig = []\n",
    "for q in Y_test:\n",
    "    if q == 0:\n",
    "        h_true_bkg.append((X_test['mll_12'].iloc[m])/1000.)\n",
    "    else:    \n",
    "        h_true_sig.append((X_test['mll_12'].iloc[m])/1000.)\n",
    "    m += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "h_pred_bkg = []\n",
    "h_pred_sig = []\n",
    "for p in y_pred:\n",
    "    if p == 0:\n",
    "        h_pred_bkg.append((X_test['mll_12'].iloc[n])/1000.)\n",
    "    else:    \n",
    "        h_pred_sig.append((X_test['mll_12'].iloc[n])/1000.)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = []\n",
    "for i in range(0,500):\n",
    "    bins.append(i*20)\n",
    "plt.hist(h_pred_sig,bins=bins,log=True, color = 'b',alpha=0.3)\n",
    "plt.hist(h_true_sig,bins=bins,log=True, color = 'r', alpha=0.3)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(h_true_bkg,bins=100,log=True)\n",
    "plt.hist(h_pred_bkg,bins=100,log=True)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "An example of a much simpler classification model is the [logistic regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGREG classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_pred_prob = logreg.predict_proba(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
