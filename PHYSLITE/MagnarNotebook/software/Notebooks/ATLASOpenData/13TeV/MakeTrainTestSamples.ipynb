{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When running this notebook via the Galaxy portal\n",
    "You can access your data via the dataset number. Using a Python kernel, you can access dataset number 42 with ``handle = open(get(42), 'r')``.\n",
    "To save data, write your data to a file, and then call ``put('filename.txt')``. The dataset will then be available in your galaxy history.\n",
    "<br><br>Note that if you are putting/getting to/from a different history than your default history, you must also provide the history-id.\n",
    "<br><br>More information including available galaxy-related environment variables can be found at https://github.com/bgruening/docker-jupyter-notebook. This notebook is running in a docker container based on the Docker Jupyter container described in that link.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))\n",
    "#%jsroot on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import import_ipynb\n",
    "import setPath\n",
    "from Input.OpenDataPandaFramework13TeV import *\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcdir = \"/storage/shared/software/Input/MC/hdf5/\"\n",
    "datadir = \"/storage/shared/software/Input/Data/hdf5/\"\n",
    "files_exist = True\n",
    "if not path.isdir(mcdir) or not path.isdir(datadir):\n",
    "    print(\"Can not find hdf5 files. Have you run ConvertNtupToHdf5.ipynb?\")\n",
    "    files_exist = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not files_exist:\n",
    "    print(\"#\"*100)\n",
    "    print(\"WARNING \\t Can not find hdf5 files. Have you run ConvertNtupToHdf5.ipynb? Make sure these file exists before continuing...\")\n",
    "    print(\"#\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skimtag = \"_4L_\"\n",
    "\n",
    "rBackgroundEvents = 0.5\n",
    "rSignalEvents = 0.5\n",
    "\n",
    "Backgrounds = getBkgCategories()\n",
    "Signals = getSignalCategories()\n",
    "\n",
    "Backgrounds.remove('Wjetsincl')\n",
    "Backgrounds.remove('Zjetsincl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_files = [f for f in listdir(mcdir) if (f.endswith('.h5') and (skimtag in f) and not ('testing_' in f or ('training' in f)))]\n",
    "print(\"Will load the following {:d} files:\\n\\t{:s}\".format(len(root_files),\"\\n\\t\".join(sorted(root_files))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfile = 0\n",
    "nx = 0\n",
    "\n",
    "# Clean the files \n",
    "onlyfiles = [f for f in listdir(mcdir) if isfile(join(mcdir, f)) and \n",
    "                                        (f.endswith(\".h5\") and \n",
    "                                        (f.startswith(\"testing_\") or f.startswith(\"training_\")))]\n",
    "for of in onlyfiles:\n",
    "    os.remove(mcdir+\"/\"+of)\n",
    "\n",
    "# Read all the root files with a given skim\n",
    "for f in root_files:\n",
    "    \n",
    "    print(\"INFO  \\t Opening file {:d}/{:d}: {:s}\".format(nfile+1,len(root_files),f))\n",
    "    df = pd.read_hdf(mcdir+\"/\"+f, 'mini')\n",
    "    \n",
    "    # Find the unique DSIDs in the file\n",
    "    dsid = np.unique(df.iloc[:,[df.columns.get_loc('channelNumber')]].to_numpy())\n",
    "    #dsid = np.unique(df.iloc[:,[1]].to_numpy())\n",
    "    # Loop over each DSID and put random selections into training and testing sample \n",
    "    nx = 0\n",
    "    for ids in dsid:\n",
    "        print(\"Doing DSID {:f} i.e. num {:d}. In this file: {:d} new DSIDs\".format(int(ids),nx+1,len(dsid)))\n",
    "        newdf = df.loc[df['channelNumber'] == ids]\n",
    "        \n",
    "        cat = np.unique(newdf.iloc[:,[newdf.columns.get_loc('MCType')]].to_numpy())\n",
    "        \n",
    "        if len(cat) > 1:\n",
    "            print(\"ERROR \\t More than one type (%s) for dsid %s\"%(\",\".join(cat),ids))\n",
    "            continue\n",
    "        cat = cat[0]\n",
    "        \n",
    "        print(\"cat = %s\"%cat)\n",
    "        # If X_test/train exists: concatenate, \n",
    "        # If not (i.e. we just wrote to a file): start new ones\n",
    "        try:\n",
    "            midl = newdf.sample(frac=rBackgroundEvents)\n",
    "            X_train = pd.concat([X_train,midl],axis=0)\n",
    "            X_test  = pd.concat([X_test, newdf.drop(midl.index.values)],axis=0)\n",
    "            del [midl]\n",
    "        except:\n",
    "            X_train = newdf.sample(frac=rBackgroundEvents)\n",
    "            X_test  = newdf.drop(X_train.index.values)\n",
    "        del [newdf]\n",
    "        nx += 1\n",
    "        # Dump testing/training samples to file every now and then (here: every tenth DSID)\n",
    "        if nx%10 == 0:\n",
    "            path = mcdir+\"/testing_%s_%s.h5\"%(cat,skimtag)\n",
    "            print(\"->Writing to file {:s}\".format(path))\n",
    "            X_test.to_hdf(path,key='result', mode='a')\n",
    "            path = mcdir+\"/training_%s_%s.h5\"%(cat,skimtag)\n",
    "            print(\"->Writing to file {:s}\".format(path))\n",
    "            X_train.to_hdf(path,key='result', mode='a')\n",
    "            del [X_test]\n",
    "            del [X_train]\n",
    "    nfile += 1\n",
    "    #if nfile > 2: break\n",
    "    del [df]\n",
    "    \n",
    "    # Needed in case we left without writing the last DSIDs to file\n",
    "    if nx%10 != 0:\n",
    "        path = mcdir+\"/testing_%s_%s.h5\"%(cat,skimtag)\n",
    "        print(\"<-Writing to file {:s}\".format(path))\n",
    "        X_test.to_hdf(path,key='result', mode='a')\n",
    "        path = mcdir+\"/training_%s_%s.h5\"%(cat,skimtag)\n",
    "        print(\"<-Writing to file {:s}\".format(path))\n",
    "        X_train.to_hdf(path,key='result', mode='a')\n",
    "        del [X_test]\n",
    "        del [X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/scratch/eirikgr/openData_13TeV/2lep/MC//hdf5//\n",
    "df = pd.read_hdf(mcdir+\"/testing_SUSYC1N2_2L_pt25_25_met50.h5\", 'result')\n",
    "col = df.columns\n",
    "print(col)\n",
    "\n",
    "df.iloc[:,[df.columns.get_loc('wgt')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/scratch/eirikgr/openData_13TeV/2lep/MC//hdf5//\n",
    "df = pd.read_hdf(mcdir+\"/training_SUSYC1N2_2L_pt25_25_met50.h5\", 'result')\n",
    "col = df.columns\n",
    "print(col)\n",
    "\n",
    "df.iloc[:,[df.columns.get_loc('wgt')]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "The following cells show an example on how to plot the variables stored in the data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we retrieve the name of all the training and testing files just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_files = [f for f in listdir(mcdir) if (f.endswith('.h5') and (f.startswith(\"testing\") and skimtag in f))]\n",
    "print(\"TESTING:  Will load the following {:d} files:\\n\\t{:s}\".format(len(testing_files),\"\\n\\t\".join(sorted(testing_files))))\n",
    "training_files = [f for f in listdir(mcdir) if (f.endswith('.h5') and (f.startswith(\"training\") and skimtag in f))]\n",
    "print(\"TRAINING: Will load the following {:d} files:\\n\\t{:s}\".format(len(training_files),\"\\n\\t\".join(sorted(training_files))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the files. If we want to load the whole set (test+trainin) or only one of them can be specified in *load_files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfile = 0\n",
    "load_files = training_files\n",
    "try:\n",
    "    del [X_train]\n",
    "except:\n",
    "    print(\"X_train does not exists yet...\")\n",
    "for f in load_files:\n",
    "    print(\"INFO  \\t Opening file {:d}/{:d}: {:s}\".format(nfile+1,len(load_files),f))\n",
    "    df = pd.read_hdf(mcdir+\"/\"+f, 'result')\n",
    "    try:\n",
    "        X_train = pd.concat([X_train,df],axis=0)\n",
    "    except:\n",
    "        X_train = df\n",
    "    del [df]\n",
    "    nfile += 1\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfile = 0\n",
    "load_files = testing_files\n",
    "try:\n",
    "    del [X_test]\n",
    "except:\n",
    "    print(\"X_test does not exists yet...\")\n",
    "for f in load_files:\n",
    "    print(\"INFO  \\t Opening file {:d}/{:d}: {:s}\".format(nfile+1,len(load_files),f))\n",
    "    df = pd.read_hdf(mcdir+\"/\"+f, 'result')\n",
    "    try:\n",
    "        X_test = pd.concat([X_test,df],axis=0)\n",
    "    except:\n",
    "        X_test = df\n",
    "    del [df]\n",
    "    nfile += 1\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.loc[X_test['isSignal'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data\n",
    "data_files = [f for f in listdir(datadir) if (f.endswith('.h5') and f.startswith(\"data\"+skimtag))]\n",
    "print(\"Will load the following {:d} file(s) for data:\\n\\t{:s}\".format(len(data_files),\"\\n\\t\".join(sorted(data_files))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notuse = [\"Wjetsincl\",\"Zjetsincl\"]\n",
    "#Backgrounds = []\n",
    "#for tf in testing_files:\n",
    "#    key = tf.split(\"_\")[1]\n",
    "#    if key in notuse: continue\n",
    "#    if key in Signals: continue\n",
    "#    Backgrounds.append(key)\n",
    "#Backgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the data (not strictly needed if only 1 file)\n",
    "nfile = 0\n",
    "for f in data_files:\n",
    "    print(\"INFO  \\t Opening file {:d}/{:d}: {:s}\".format(nfile+1,len(data_files),f))\n",
    "    df = pd.read_hdf(datadir+\"/\"+f, 'mini')\n",
    "    try:\n",
    "        X_data = pd.concat([X_data,df],axis=0)\n",
    "    except:\n",
    "        X_data = df\n",
    "    del [df]  \n",
    "    nfile += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some plotting specific setting (order of plotting, color of backgrounds).The *stack_order* must have the same keys as in the *MCType* column in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_order = ['Data'] + Backgrounds + [\"Gee\"]\n",
    "bkgs = X_train['MCType'].unique()\n",
    "for s in stack_order:\n",
    "    if \"Data\" in s: continue\n",
    "    if not s in bkgs: print(\"ERROR \\t Key {:s} is not in panda\".format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSamplesInCategory(\"Gee\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = {}\n",
    "for s in stack_order:\n",
    "    if not s in sf.keys():\n",
    "        sf[s] = {\"train\":0,\"test\":0}\n",
    "    print(s)\n",
    "    if s in Backgrounds:\n",
    "        rslt_df_train = X_train.loc[X_train['MCType'] == s]\n",
    "        rslt_df_test  = X_test.loc[X_test['MCType'] == s]\n",
    "        train_sum = rslt_df_train['wgt'].sum()\n",
    "        test_sum  = rslt_df_test['wgt'].sum()\n",
    "        print(\"Train: %s %s\"%(s,train_sum))\n",
    "        print(\"Test : %s %s\"%(s,test_sum))\n",
    "        sf[s][\"test\"] = (train_sum+test_sum)/test_sum\n",
    "        sf[s][\"train\"] = (train_sum+test_sum)/train_sum\n",
    "        #print(sf[s][\"test\"])\n",
    "        #print(sf[s][\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the numpy arrays from the panda data frame (specify the variable of interest in *var*). Here the limits, bin width etc. are set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_mll = []\n",
    "mc_weights = []\n",
    "mc_colors = []\n",
    "mc_labels = []\n",
    "\n",
    "data_mll = []\n",
    "data_mll_errors = []\n",
    "\n",
    "sig_mll = []\n",
    "sig_mll_errors = []\n",
    "sig_weights = []\n",
    "\n",
    "signal_dsid = 341122\n",
    "\n",
    "var = \"lep1_pt\"\n",
    "top = -999\n",
    "\n",
    "nmax = 1000\n",
    "nmin = 0\n",
    "binw = 20\n",
    "\n",
    "data_x = []\n",
    "if not ((nmax-nmin)/binw).is_integer():\n",
    "    print(\"ERROR \\t Limits and bin width are not compatible\")\n",
    "#print(int((nmax-nmin)/binw)+1)\n",
    "bins = [nmin + (x*binw) for x in range(int((nmax-nmin)/binw)+1)]\n",
    "for i in range(len(bins)-1):\n",
    "    #print(bins[i])\n",
    "    data_x.append(bins[i]+(bins[i+1]-bins[i])/2)\n",
    "#data_x = [((nmin+1) + x*binw) for x in range(int((nmax-nmin)/binw)) ]\n",
    "\n",
    "for s in stack_order:\n",
    "    if s == \"Data\":\n",
    "        data_mll,_ = np.histogram(X_data[X_data.columns[X_data.columns.get_loc(var):X_data.columns.get_loc(var)+1]]/1000.,bins=bins)\n",
    "        #data_mll,_ = np.histogram(X_data.as_matrix(columns=X_data.columns[X_data.columns.get_loc(var):X_data.columns.get_loc(var)+1])/1000., bins=bins)\n",
    "        data_mll_errors = np.sqrt(data_mll)\n",
    "    elif s in Signals:\n",
    "        rslt_df = X_train.loc[X_train['channelNumber'] == signal_dsid]\n",
    "        sig_mll.append(rslt_df.iloc[:,[rslt_df.columns.get_loc(var)]].to_numpy()/1000.)\n",
    "        sig_weights.append(rslt_df.iloc[:,[rslt_df.columns.get_loc(\"wgt\")]].to_numpy()/1000.)\n",
    "    elif s in Backgrounds:\n",
    "        rslt_df = X_train.loc[X_train['MCType'] == s]\n",
    "        mc_mll.append(rslt_df.iloc[:,[rslt_df.columns.get_loc(var)]].to_numpy()/1000.)\n",
    "        mc_weights.append(rslt_df.iloc[:,[rslt_df.columns.get_loc(\"wgt\")]].to_numpy()/1000.)\n",
    "        #mc_mll.append(rslt_df[rslt_df.columns[rslt_df.columns.get_loc(var):rslt_df.columns.get_loc(var)+1]].to_numpy()/1000)\n",
    "        #mc_weights.append(rslt_df[rslt_df.columns[rslt_df.columns.get_loc(\"wgt\"):rslt_df.columns.get_loc(\"wgt\")+1]].to_numpy()*1000)\n",
    "        #mc_mll.append(rslt_df.as_matrix(columns=rslt_df.columns[rslt_df.columns.get_loc(var):rslt_df.columns.get_loc(var)+1])/1000.)\n",
    "        #mc_weights.append(rslt_df.as_matrix(columns=rslt_df.columns[rslt_df.columns.get_loc(\"wgt\"):rslt_df.columns.get_loc(\"wgt\")+1])*(1000.))\n",
    "        mc_colors.append(bkg_plot_dic[s]['color'])\n",
    "        mc_labels.append(s)\n",
    "        if np.amax(mc_mll[-1]) > top:\n",
    "            top = np.amax(mc_mll[-1])\n",
    "        del [rslt_df]\n",
    "        \n",
    "\n",
    "mc_mll_array = np.array(mc_mll,dtype='object')\n",
    "mc_weights_array = np.array(mc_weights,dtype='object')\n",
    "\n",
    "sig_mll_array = np.array(sig_mll,dtype='float32')[0]\n",
    "sig_weights_array = np.array(sig_weights,dtype='float32')[0]\n",
    "data_mll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, do the plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "figure(num=None, figsize=(12, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.hist(x=mc_mll_array,bins=bins,weights=mc_weights_array,stacked=True, label=mc_labels); #weights=mc_weights,color=mc_colors,\n",
    "plt.errorbar( x=data_x, y=data_mll, yerr=data_mll_errors, fmt='ko', label='Data')\n",
    "plt.hist(x=sig_mll_array,bins=bins,stacked=True,weights=sig_weights_array, label=['Signal'])\n",
    "plt.yscale('log')\n",
    "plt.ylabel(r'Events',fontname='sans-serif',horizontalalignment='right',y=1.0,fontsize=11)\n",
    "plt.xlabel(r'$M_{ll}$ [GeV]',fontname='sans-serif',horizontalalignment='right',x=1.0,fontsize=11)\n",
    "\n",
    "plt.ylim(bottom=0.001,top=50000)#top/40.)\n",
    "\n",
    "ax = plt.gca()\n",
    "plt.text(0.05,0.97,r'$\\mathbf{{ATLAS}}$ Open Data',ha=\"left\",va=\"top\",family='sans-serif',transform=ax.transAxes,fontsize=13)\n",
    "plt.text(0.05,0.92,'for education only',ha=\"left\",va=\"top\",family='sans-serif',transform=ax.transAxes,style='italic',fontsize=8)\n",
    "plt.text(0.05,0.90,r'$\\sqrt{s}=13\\,\\mathrm{TeV},\\;\\int L\\,dt=10\\,\\mathrm{fb}^{-1}$',ha=\"left\",va=\"top\",family='sans-serif',transform=ax.transAxes)\n",
    "\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
